{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rskamit11/CAPESTONE-EDA-PROJECT/blob/main/ZOMATO_UNSUPERVISED_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n",
        "ZOMATO RESTAURANT CLUSTERING AND SENTIMENTAL ANALYSIS.\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**  AMIT S KASHYAP\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words.\n",
        "\n",
        "ndia is quite famous for its diverse multi cuisine available in a large number of restaurants and hotel resorts, which is reminiscent of unity in diversity. Restaurant business in India is always evolving. More Indians are warming up to the idea of eating restaurant food whether by dining outside or getting food delivered. The growing number of restaurants in every state of India has been a motivation to inspect the data to get some insights, interesting facts and figures about the Indian food industry in each city. So, this project focuses on analysing the Zomato restaurant data for each city in India.\n",
        "\n",
        "There are two separate files, while the columns are self explanatory. Below is a brief description:\n",
        "\n",
        "Restaurant names and Metadata - This could help in clustering the restaurants into segments. Also the data has valuable information around cuisine and costing which can be used in cost vs. benefit analysis Restaurant reviews - Data could be used for sentiment analysis. Also the metadata of reviewers can be used for identifying the critics in the industry.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here.\n",
        "\n",
        "https://github.com/rskamit11/ZOMATO-UNSUPERVISED-CAPESTONE-PROJECT/tree/main"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**\n",
        "\n",
        "e Project focuses on Customers and Company, you have to analyze the sentiments of the reviews given by the customer in the data and made some useful conclusion in the form of Visualizations. Also, cluster the zomato restaurants into different segments. The data is vizualized as it becomes easy to analyse data at instant. The Analysis also solve some of the business cases that can directly help the customers finding the Best restaurant in their locality and for the company to grow up and work on the fields they are currently lagging in.\n",
        "\n",
        "This could help in clustering the restaurants into segments. Also the data has valuable information around cuisine and costing which can be used in cost vs. benefit analysis\n",
        "\n",
        "Data could be used for sentiment analysis. Also the metadata of reviewers can be used for identifying the critics in the industry.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import math\n",
        "import time\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "#importing kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#importing random forest and XgB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#Non-negative matrix Factorization\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#principal component analysis\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "#silhouette score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "#importing stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "#for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "# for POS tagging(Part of speech in NLP sentiment analysis)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#import stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "#import tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#importing contraction\n",
        "!pip install contractions\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "#importing shap for model explainability\n",
        "!pip install shap\n",
        "import shap\n",
        "\n",
        "#download small spacy model\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# import spacy\n",
        "\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.float_format = \"{:.2f}\".format\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "HPantvqzmE-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df = pd.read_csv('/content/Zomato Restaurant names and Metadata.csv')"
      ],
      "metadata": {
        "id": "PlyylG6aoln6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df =pd.read_csv('/content/Zomato Restaurant reviews.csv')"
      ],
      "metadata": {
        "id": "C8jYhT2bowza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df.head()"
      ],
      "metadata": {
        "id": "rwfcj2ojpDWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.head()"
      ],
      "metadata": {
        "id": "4bvFPi8lpPgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df.shape"
      ],
      "metadata": {
        "id": "qZrZDRgJpf6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.shape"
      ],
      "metadata": {
        "id": "HsxOpApapj7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df.info()\n",
        "print('='*120)\n",
        "review_df.info()"
      ],
      "metadata": {
        "id": "JmyUhCU9j7uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df.duplicated().value_counts()\n"
      ],
      "metadata": {
        "id": "9cRY5dD2WvuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.duplicated().value_counts()"
      ],
      "metadata": {
        "id": "L-o4s_PNqvDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting duplicate values\n",
        "print(f' Duplicate data count = {review_df[review_df.duplicated()].shape[0]}')\n",
        "review_df[review_df.duplicated()]\n"
      ],
      "metadata": {
        "id": "aUwunqTtrGf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking values for Anerican Wild Things\n",
        "review_df[(review_df['Restaurant'] == 'American Wild Wings')].shape\n"
      ],
      "metadata": {
        "id": "z0uR9g0hrVtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking values for Arena Eleven\n",
        "review_df[(review_df['Restaurant'] == 'Arena Eleven')].shape\n"
      ],
      "metadata": {
        "id": "LVjncSMRrZ59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count for hotel data\n",
        "meta_df.isnull().sum()"
      ],
      "metadata": {
        "id": "C7_5o97Iru_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count for review data\n",
        "review_df.isnull().sum()"
      ],
      "metadata": {
        "id": "rpWrnzUCr28U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(meta_df.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "bwZkOunIskng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(review_df.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "44Di5tqGsrsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "META DATA :-\n",
        "There are 105 total observation with 6 different features.\n",
        "Feature like collection and timing has null values.\n",
        "There is no duplicate values i.e., 105 unique data.\n",
        "Feature cost represent amount but has object data type because these values are separated by comma ','.\n",
        "Timing represent operational hour but as it is represented in the form of text has object data type.\n",
        "\n",
        "REVIEW DATA :-\n",
        "There are total 10000 observation and 7 features.\n",
        "Except picture and restaurant feature all others have null values.\n",
        "There are total of 36 duplicate values for two restaurant - American Wild Wings and Arena Eleven, where all these duplicate values generally have null values.\n",
        "Rating represent ordinal data, has object data type should be integer.\n",
        "Timing represent the time when review was posted but show object data time, it should be converted into date time."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "meta_df.columns.to_list()"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.columns.to_list()"
      ],
      "metadata": {
        "id": "NQiAhw2NthgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "meta_df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.describe(include='all').T"
      ],
      "metadata": {
        "id": "ngyx3pVytvmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Zomato Restaurant names and Metadata\n",
        "\n",
        "Name : Name of Restaurants\n",
        "\n",
        "Links : URL Links of Restaurants\n",
        "\n",
        "Cost : Per person estimated Cost of dining\n",
        "\n",
        "Collection : Tagging of Restaurants w.r.t. Zomato categories\n",
        "\n",
        "Cuisines : Cuisines served by Restaurants\n",
        "\n",
        "Timings : Restaurant Timings\n",
        "\n",
        "Zomato Restaurant reviews\n",
        "\n",
        "Restaurant : Name of the Restaurant\n",
        "\n",
        "Reviewer : Name of the Reviewer\n",
        "\n",
        "Review : Review Text\n",
        "\n",
        "Rating : Rating Provided by Reviewer\n",
        "\n",
        "MetaData : Reviewer Metadata - No. of Reviews and followers\n",
        "\n",
        "Time: Date and Time of Review\n",
        "\n",
        "Pictures : No. of pictures posted with review\n",
        "\n",
        "Zomato Restaurant Reviews\n",
        "\n",
        "Restaurant : Name of the Restaurant\n",
        "\n",
        "Reviewer : Name of the Reviewer\n",
        "\n",
        "Review : Review Text\n",
        "\n",
        "Rating : Rating Provided by Reviewer\n",
        "\n",
        "MetaData : Reviewer Metadata - No. of Reviews and followers\n",
        "\n",
        "Time: Date and Time of Review\n",
        "\n",
        "Pictures : No. of pictures posted with review"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in meta_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",meta_df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "2l_hDTDFuxGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in review_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",review_df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "VQk9h4_3vAOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#creating copy of both the data\n",
        "meta = meta_df.copy()\n",
        "review = review_df.copy()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#before changing data type for cost checking values\n",
        "meta['Cost'].unique()\n"
      ],
      "metadata": {
        "id": "yE2zqt0SvoJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing the data type of the cost function\n",
        "meta['Cost'] = meta['Cost'].str.replace(\",\",\"\").astype('int64')"
      ],
      "metadata": {
        "id": "GbDArSMnvt6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 costlier restaurant\n",
        "meta.sort_values('Cost', ascending = False)[['Name','Cost']][:5]\n"
      ],
      "metadata": {
        "id": "amPhB38xwCTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 economy restaurant\n",
        "meta.sort_values('Cost', ascending = False)[['Name','Cost']][-5:]"
      ],
      "metadata": {
        "id": "Aerl4uEDwLOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hotels that share same price\n",
        "meta_dict = {}\n",
        "amount = meta.Cost.values.tolist()\n",
        "\n",
        "#adding meta name based on the price by converting it into list\n",
        "for price in amount:\n",
        "    # Get all the rows that have the current price\n",
        "    rows = meta[meta['Cost'] == price]\n",
        "    meta_dict[price] = rows[\"Name\"].tolist()\n",
        "\n",
        "#converting it into dataframe\n",
        "same_price_meta_df=pd.DataFrame.from_dict([meta_dict]).transpose().reset_index().rename(\n",
        "    columns={'index':'Cost',0:'Name of Restaurants'})\n",
        "\n",
        "#alternate methode to do the same\n",
        "#same_price_hotel_df = hotel.groupby('Cost')['Name'].apply(lambda x: x.tolist()).reset_index()\n",
        "\n",
        "#getting hotel count\n",
        "hotel_count = meta.groupby('Cost')['Name'].count().reset_index().sort_values(\n",
        "    'Cost', ascending = False)\n",
        "\n",
        "#merging together\n",
        "same_price_meta_df = same_price_meta_df.merge(hotel_count, how = 'inner',\n",
        "                        on = 'Cost').rename(columns = {'Name':'Total_Restaurant'})\n",
        "\n",
        "#max hotels that share same price\n",
        "same_price_meta_df.sort_values('Total_Restaurant', ascending = False)[:5]"
      ],
      "metadata": {
        "id": "TNh3Dc3pweZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hotels which has max price\n",
        "same_price_meta_df.sort_values('Cost', ascending = False)[:5]\n"
      ],
      "metadata": {
        "id": "uWcQ1XC6xyGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the cusines and storing in list\n",
        "cuisine_value_list = meta.Cuisines.str.split(', ')"
      ],
      "metadata": {
        "id": "sAw2CVO9x_Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the cusines in a dict\n",
        "cuisine_dict = {}\n",
        "for cuisine_names in cuisine_value_list:\n",
        "    for cuisine in cuisine_names:\n",
        "        if (cuisine in cuisine_dict):\n",
        "            cuisine_dict[cuisine]+=1\n",
        "        else:\n",
        "            cuisine_dict[cuisine]=1"
      ],
      "metadata": {
        "id": "_sVTQjrUyJyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the dict to a data frame\n",
        "cuisine_df=pd.DataFrame.from_dict([cuisine_dict]).transpose().reset_index().rename(\n",
        "    columns={'index':'Cuisine',0:'Number of Restaurants'})"
      ],
      "metadata": {
        "id": "p7sv62rmyTux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 cuisine\n",
        "cuisine_df.sort_values('Number of Restaurants', ascending =False)[:5]\n"
      ],
      "metadata": {
        "id": "VkPkaLyDygG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the cusines and storing in list\n",
        "Collections_value_list = meta.Collections.dropna().str.split(', ')"
      ],
      "metadata": {
        "id": "bLAXgrpwysGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the cusines in a dict\n",
        "Collections_dict = {}\n",
        "for collection in Collections_value_list:\n",
        "    for col_name in collection:\n",
        "        if (col_name in Collections_dict):\n",
        "            Collections_dict[col_name]+=1\n",
        "        else:\n",
        "            Collections_dict[col_name]=1\n"
      ],
      "metadata": {
        "id": "3UJNMTt8yxYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the dict to a data frame\n",
        "Collections_df=pd.DataFrame.from_dict([Collections_dict]).transpose().reset_index().rename(\n",
        "    columns={'index':'Tags',0:'Number of Restaurants'})\n"
      ],
      "metadata": {
        "id": "ZEhR8H8Ly7uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 collection\n",
        "Collections_df.sort_values('Number of Restaurants', ascending =False)[:5]\n"
      ],
      "metadata": {
        "id": "aHpLl_7IzIrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REVIEWS DATA"
      ],
      "metadata": {
        "id": "q5Ic-5HRzbS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in order to change data type for rating checking values\n",
        "review.Rating.value_counts()"
      ],
      "metadata": {
        "id": "pt1XJO_Azf-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing data type for each rating since had value as interger surrounded by inverted comma\n",
        "#since there is one rating as like converting it to 0 since no rating is 0 then to median\n",
        "review.loc[review['Rating'] == 'Like'] = 0\n",
        "#changing data type for rating in review data\n",
        "review['Rating'] = review['Rating'].astype('float')"
      ],
      "metadata": {
        "id": "ZqCzO4dAzqug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since there is one rating as like converting it to median\n",
        "review.loc[review['Rating'] == 0] = review.Rating.median()"
      ],
      "metadata": {
        "id": "ziOZ_vSfzufK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing date and extracting few feature for manipulation\n",
        "\n",
        "review['Reviewer_Total_Review'],review['Reviewer_Followers']=review['Metadata'].str.split(',').str\n",
        "review['Reviewer_Total_Review'] = pd.to_numeric(review['Reviewer_Total_Review'].str.split(' ').str[0])\n",
        "review['Reviewer_Followers'] = pd.to_numeric(review['Reviewer_Followers'].str.split(' ').str[1])\n",
        "review['Time']=pd.to_datetime(review['Time'])\n",
        "review['Review_Year'] = pd.DatetimeIndex(review['Time']).year\n",
        "review['Review_Month'] = pd.DatetimeIndex(review['Time']).month\n",
        "review['Review_Hour'] = pd.DatetimeIndex(review['Time']).hour\n"
      ],
      "metadata": {
        "id": "fLQULSXCz6mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Average engagement of restaurants\n",
        "avg_hotel_rating = review.groupby('Restaurant').agg({'Rating':'mean',\n",
        "        'Reviewer': 'count'}).reset_index().rename(columns = {'Reviewer': 'Total_Review'})\n",
        "avg_hotel_rating"
      ],
      "metadata": {
        "id": "5lknAvYD0KXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#usless data\n",
        "review[review['Restaurant'] == 4.0]\n"
      ],
      "metadata": {
        "id": "0aUTPdbF0o7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking hotel count as total hotel in restaurant data was 105\n",
        "review.Restaurant.nunique()\n"
      ],
      "metadata": {
        "id": "cW4YfOzg0tgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding hotel without review\n",
        "hotel_without_review = [name for name in meta.Name.unique().tolist()\n",
        "       if name not in review.Restaurant.unique().tolist()]\n",
        "hotel_without_review"
      ],
      "metadata": {
        "id": "1CUd8qoN09L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 most engaging or rated restaurant\n",
        "avg_hotel_rating.sort_values('Rating', ascending = False)[:5]\n"
      ],
      "metadata": {
        "id": "Cq57uQE61MRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 lowest rated restaurant\n",
        "avg_hotel_rating.sort_values('Rating', ascending = True)[:5]\n"
      ],
      "metadata": {
        "id": "8MH-NA8C1a0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the most followed critic\n",
        "most_followed_reviewer = review.groupby('Reviewer').agg({'Reviewer_Total_Review':'max',\n",
        "      'Reviewer_Followers':'max', 'Rating':'mean'}).reset_index().rename(columns = {\n",
        "          'Rating':'Average_Rating_Given'}).sort_values('Reviewer_Followers', ascending = False)\n",
        "most_followed_reviewer[:5]"
      ],
      "metadata": {
        "id": "SGFecs1T1pQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding which year show maximum engagement\n",
        "hotel_year = review.groupby('Review_Year')['Restaurant'].apply(lambda x: x.tolist()).reset_index()\n",
        "hotel_year['Count']= hotel_year['Restaurant'].apply(lambda x: len(x))\n",
        "hotel_year"
      ],
      "metadata": {
        "id": "XGRyvRvU11rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging both data frame\n",
        "meta = meta.rename(columns = {'Name':'Restaurant'})\n",
        "merged = meta.merge(review, on = 'Restaurant')\n",
        "merged.shape"
      ],
      "metadata": {
        "id": "-P4A8CxU2DlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Price point of restaurants\n",
        "price_point = merged.groupby('Restaurant').agg({'Rating':'mean',\n",
        "        'Cost': 'mean'}).reset_index().rename(columns = {'Cost': 'Price_Point'})\n"
      ],
      "metadata": {
        "id": "la8J2qXg2bhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#price point for high rated restaurants\n",
        "price_point.sort_values('Rating',ascending = False)[:5]\n"
      ],
      "metadata": {
        "id": "ZnwWwPLR2kc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#price point for lowest rated restaurants\n",
        "price_point.sort_values('Rating',ascending = True)[:5]\n"
      ],
      "metadata": {
        "id": "E0aBU2jy2o1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rating count by reviewer\n",
        "rating_count_df = pd.DataFrame(review.groupby('Reviewer').size(), columns=[\n",
        "                                                                \"Rating_Count\"])\n",
        "rating_count_df.sort_values('Rating_Count', ascending = False)[:5]"
      ],
      "metadata": {
        "id": "8suPMwWF25X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Restaurant data : In this dataset I first figured out 5 costlier restaurant in which Collage - Hyatt Hyderabad Gachibowli has maximum price of 2800 and then found the lowest which is Amul with price of 150. Then I found how many hotel share same price i.e., 13 hotel share 500 price. North indian cuisine with great buffet tags is mostly used in hotels.\n",
        "\n",
        "Review data : In this dataset I found famous or restaurant that show maximum engagement. Followed by that I found most followed critic which was Satwinder Singh who posted total of 186 reviews and had followers of 13410 who gives and average of 3.67 rating for each order he makes. Lastly I also found in year 2018 4903 hotels got reviews."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (18,8));\n",
        "for i,col in enumerate(['Cost','Rating','Review_Year']) :\n",
        "    # plt.figure(figsize = (8,5));\n",
        "    plt.subplot(2,2,i+1);\n",
        "    sns.distplot(merged[col], color = '#055E85', fit = norm);\n",
        "    feature = merged[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n",
        "    plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right')\n",
        "    plt.title(f'{col.title()}');\n",
        "    plt.tight_layout();"
      ],
      "metadata": {
        "id": "-HbvWMwFsnFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Displot for distribution"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "All three are show skewness.\n",
        "Maximum restaurant show price range for 500.\n",
        "In 2018 number of reviews are more."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Price always place important role in any business alongwith rating which show how much engagement are made for the product."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_engaged_hotel = price_point.sort_values('Rating', ascending = False)"
      ],
      "metadata": {
        "id": "_tkGetZLvJY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data =most_engaged_hotel [:10], x = 'Rating', y = 'Restaurant')\n",
        "plt.title('Most Engaged Restaurant')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "82Dp5FJCtoHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.  \n",
        "\n",
        "Price Point and Maximum Engagement"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "AB's - Absolute Barbecues, show maximum engagement and retention as it has maximum number of rating on average"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Engagement and retention for any business is very much important as profit and scalability for any business depend upon retention of customers. Maximum retention means people prefer to use the same brand over others."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data = most_engaged_hotel[:10], x = 'Price_Point', y = 'Restaurant')\n",
        "plt.title(' High Rated Restaurant')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SwozzaSswaQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data = most_engaged_hotel[-10:], x = 'Price_Point',\n",
        "            y = 'Restaurant',palette = 'hsv')\n",
        "plt.title('Low Rated Restaurant', size = 15)"
      ],
      "metadata": {
        "id": "k7pNIy74w_2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Price Point for High Rated and Low Rated Hotels\n",
        "\n",
        "Value counts of collection"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Price point for high rated hotel AB's= Absolute Barbecues is 1500 and price point for low rated restaurant Hotel Zara Hi-Fi is 400.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Here most liked restaurant has a price point of 1500 which is even though a little high than average but as this business is all about food quality and taste it show maximum engagement which means it serve best quality of food, however deep dive on analysing review text can exactly give why this price point is prefered most.\n",
        "\n",
        "Some restaurant with lowest rating even with low price point is not making engagement, this may create a negative impact on business."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wordcloud for Cuisine\n",
        "# storind all cuisine in form of text\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in cuisine_df.Cuisine )\n",
        "\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 2000, height = 2000,collocations = False,\n",
        "                       colormap='rainbow',background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "plt.imshow(word_cloud, interpolation='bilinear');\n",
        "\n",
        "plt.axis(\"off\");\n"
      ],
      "metadata": {
        "id": "b1Ek44EnxvF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Word cloud for cuisine."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Based on the above chart it is clear that most of the hotel sold North Indian food followed by chinese."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Identifying commoditized cuisines can also provide insight into consumer preferences, which can be used to make informed decisions about menu offerings, pricing, and promotions.\n",
        "\n",
        "This information can be helpful in understanding the most common cuisine types in the dataset. It can also be used to identify trends and patterns in the types of cuisines that are popular or in demand among the customers."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all cuisine in form of text\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in Collections_df.Tags )\n",
        "\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 1400, height = 1400,collocations = False,\n",
        "                      colormap='rainbow', background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "plt.imshow(word_cloud, interpolation='bilinear');\n",
        "\n",
        "plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "VN5FZLMhz8is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "A word cloud displays the most frequently mentioned attributes in a way that is visually striking and easy to understand. It is useful for identifying the most frequently mentioned attributes and can be used to quickly identify patterns and trends in customer reviews."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Great Buffets is the most frequently used tags and other tags like great, best, north, Hyderabad is also used in large quantity.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "A word cloud of tags used to describe food can help Zomato identify the most frequently mentioned food attributes in customer reviews. This can provide insight into which attributes are most popular and well-regarded among customers, and which attributes may need improvement."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data = most_followed_reviewer[:10], x = 'Reviewer_Total_Review',\n",
        "            y = 'Reviewer', palette='bright')\n",
        "plt.title('Maximum Review')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qSmG5eS61hRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Most review by Reviewer."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Satwinder singh is the most popular critic who has maximum number of follower."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "It's important to note that this chart does not provide all the information about the business, and can not be the only decision making factor. However it can help on promotions food based on reviews."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_of_hotel = meta.sort_values('Cost', ascending = False)[['Restaurant','Cost']]"
      ],
      "metadata": {
        "id": "3JGv5uvI3xoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging average rating and cost to find rating for expensive hotel\n",
        "expected_revenue = avg_hotel_rating.merge(meta[['Restaurant','Cost']], on = 'Restaurant')\n",
        "#calculating expected revenue based on total review recieved\n",
        "expected_revenue['Expected_Revenue'] = expected_revenue['Total_Review'] * expected_revenue['Cost']\n"
      ],
      "metadata": {
        "id": "GwMqA58a3Gve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,6))\n",
        "data = expected_revenue.sort_values('Cost', ascending  = False)\n",
        "sns.scatterplot(data= data, x= \"Restaurant\", y=\"Rating\", size=\"Cost\",\n",
        "                hue = 'Cost',legend=True, sizes=(20, 2000),palette =\"icefire\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Restaurants Price vs Ratings',size=20,color = 'red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OPfCD6nA5Wkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Restaurant on the basis of price and ratings."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Based on the above chart it is clear that restaurant Collage - Hyatt Hyderabad Gachibowli is most expensive restaurant in the locality which has a price of 2800 for order and has 3.5 average rating."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Based on the average rating of 3.4 these product should increase their engagement as this may cause negative brand impact. However true behaviour can only be inspected through analysing of reviews."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=[20,6])\n",
        "sns.barplot(data= data, x='Restaurant', y= 'Expected_Revenue', palette =\"rainbow\")\n",
        "plt.title(\"Expected_Revenue from each Restaurant\", size = 22)\n",
        "plt.xlabel('Restaurant Name', size = 22)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Expected_Revenue', size = 22)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u117rYhI-OPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To understand expected revenue."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Based on the above chart it is clear that restaurant Collage - Hyatt Hyderabad Gachibowli is most expensive restaurant in the locality which has a price of 2800 for order."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Most expensive product are always center of attraction for a niche market (subset of the market on which a specific product is focused) at the same time for a business purpose, this product are preffered to be most revenue generating market."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Word cloud for positive reviews."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Service, taste, time, starters are key to good review."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "These gives an perespective of customers about a services and meals, which helps to desirable changes accordingly And ultimately it gives an positive results."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Negative reviews wordcloud."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Service , bad chicken , staff behavior, stale food are key reasons for neagtive reviews"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "As we discussed in above about positive impact this will also an provide us important information about what the customers views about restaurant."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(figsize = (20, 10))\n",
        "sns.heatmap(merged.corr(),ax = ax, annot=True, cmap = 'icefire', linewidths = 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0BwMSCJu_Uvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        " A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1]."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        " It can be depicted that few features are correlated, like reviewer total review is related to reviewer follower and again reviewer total review is related to pictures."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(merged);"
      ],
      "metadata": {
        "id": "Or6Um49F_5cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Null hypothesis: There is no relationship between the cost of restaurant and the rating it receives. (H0: 1 = 0)\n",
        "Alternative hypothesis: There is a positive relationship between the cost of a restaurant and the rating it receives. (H1: 1 > 0)\n",
        "Test : Simple Linear Regression Analysis"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "\n",
        "# fit the linear model\n",
        "model = smf.ols(formula='Rating ~ Cost', data= merged).fit()\n",
        "\n",
        "# Check p-value of coefficient\n",
        "p_value = model.pvalues[1]\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis - There is no relationship between the cost of\\\n",
        " restaurant and the rating it receives.\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis - There is a positive relationship \\\n",
        " between the cost of a restaurant and the rating it receives.\")"
      ],
      "metadata": {
        "id": "tTd41qXDBENR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I have used Linear regression test for checking the relationship between the cost of a restaurant and its rating"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The p-value of the coefficient for the cost variable can then be used to determine if there is a statistically significant relationship between the two variables."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Restaurants that are reviewed by reviewers with more followers will have a higher rating."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# fit the linear model\n",
        "model = smf.ols(formula='Rating ~ Reviewer_Followers', data = merged).fit()\n",
        "\n",
        "# print the summary of the model\n",
        "# print(model.summary())\n",
        "\n",
        "# extract p-value of coefficient for Reviewer_Followers\n",
        "p_value = model.pvalues[1]\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis\")\n"
      ],
      "metadata": {
        "id": "BRQT_nAICDmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        " I have used Simple Linear Regression Test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "It assumes that there is a linear relationship between the independent variable (Reviewer_Followers) and the dependent variable (Rating) and it allows us to estimate the strength and direction of that relationship"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Null hypothesis: The variety of cuisines offered by a restaurant has no effect on its rating. (H0: 3 = 0)\n",
        "Alternative hypothesis: The variety of cuisines offered by a restaurant has a positive effect on its rating. (H1: 3 > 0)\n",
        "Test : Chi-Squared Test"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(merged['Cuisines'], merged['Rating'])[:1]"
      ],
      "metadata": {
        "id": "Tf61W57IC8tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# create a contingency table\n",
        "ct = pd.crosstab(merged['Cuisines'], merged['Rating'])\n",
        "\n",
        "# perform chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(ct)\n",
        "\n",
        "# Check p-value\n",
        "if p < 0.05:\n",
        "    print(\"Reject Null Hypothesis\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis\")\n"
      ],
      "metadata": {
        "id": "A6zeDfStDyIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I have used chi-squared test for independence to test the relationship between the variety of cuisines offered by a restaurant and its rating."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "This would involve creating a contingency table with the number of restaurants that offer each cuisine as the rows and the rating of the restaurant as the columns."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deleting duplicate value from review dataset\n",
        "review = review.drop_duplicates()"
      ],
      "metadata": {
        "id": "odxKSMAqLyqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RESTAURANT DATA\n",
        "meta.isnull().sum()"
      ],
      "metadata": {
        "id": "cdYeX7ejMKLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the null value in timing\n",
        "meta[meta['Timings'].isnull()]"
      ],
      "metadata": {
        "id": "q5lm55LJMclL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling null value in timings column\n",
        "meta.Timings.fillna(meta.Timings.mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "zCyCRrpiMonZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking null values in Collections\n",
        "missing_percentage = ((meta['Collections'].isnull().sum())/(len(meta['Collections'])))*100\n",
        "print(f'Percentage of missing value in Collections is {round(missing_percentage, 2)}%')\n"
      ],
      "metadata": {
        "id": "yJY4NmT8MtTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping collection column since has more than 50% of null values\n",
        "meta.drop('Collections', axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "id": "k5KEbn7MM0Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta.isnull().sum()"
      ],
      "metadata": {
        "id": "hcqFZrVqM4ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REVIEW DATA\n",
        "review.isnull().sum()"
      ],
      "metadata": {
        "id": "w7hE3nbrbQ86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review[review['Reviewer'].isnull()]"
      ],
      "metadata": {
        "id": "_NFa5TmKbdS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review[review['Reviewer_Total_Review'].isnull()]"
      ],
      "metadata": {
        "id": "ONHKHUAMbkGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = review.dropna(subset=['Reviewer','Reviewer_Total_Review'])"
      ],
      "metadata": {
        "id": "x9_KvZP9bugv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#again checking the remaining values\n",
        "null_counts = [(x, a) for x, a in review.isnull().sum().items() if a > 0]\n",
        "\n",
        "# Print the columns with null values\n",
        "null_counts"
      ],
      "metadata": {
        "id": "lv3pEoE6b77g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = review.fillna({\"Review\": \"No Review\", \"Reviewer_Followers\": 0})\n"
      ],
      "metadata": {
        "id": "SHUjT49mcEzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review.isnull().sum()"
      ],
      "metadata": {
        "id": "6aADOBpecKEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging both dataset\n",
        "merged = meta.merge(review, on = 'Restaurant')\n",
        "merged.shape"
      ],
      "metadata": {
        "id": "WTNjUQSucSlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Anamoly detection\n",
        "from sklearn.ensemble import IsolationForest\n",
        "#checking for normal distribution\n",
        "print(\"Skewness - Cost: %f\" % merged['Cost'].skew())\n",
        "print(\"Kurtosis - Cost: %f\" % merged['Cost'].kurt())\n",
        "print(\"Skewness - Reviewer_Followers: %f\" % merged['Reviewer_Followers'].skew())\n",
        "print(\"Kurtosis - Reviewer_Followers: %f\" % merged['Reviewer_Followers'].kurt())\n"
      ],
      "metadata": {
        "id": "OGZhkUxAchpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(range(merged.shape[0]), np.sort(merged['Cost'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Cost')\n",
        "plt.title(\"Cost distribution\")\n",
        "sns.despine()\n"
      ],
      "metadata": {
        "id": "o-Y_aFLSc9tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(merged['Cost'])\n",
        "plt.title(\"Distribution of Cost\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "uI_tuu8pdBCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(range(merged.shape[0]), np.sort(merged['Reviewer_Followers'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Reviewer_Followers')\n",
        "plt.title(\"Reviewer_Followers distribution\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "qGDA-fY2dOub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#isolation forest for anamoly detection on cost\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n",
        "isolation_forest.fit(merged['Cost'].values.reshape(-1, 1))\n",
        "merged['anomaly_score_univariate_Cost'] = isolation_forest.decision_function(merged['Cost'].values.reshape(-1, 1))\n",
        "merged['outlier_univariate_Cost'] = isolation_forest.predict(merged['Cost'].values.reshape(-1, 1))\n"
      ],
      "metadata": {
        "id": "SLo_8hJpeJ8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chart to visualize outliers\n",
        "xx = np.linspace(merged['Cost'].min(), merged['Cost'].max(), len(merged)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Cost')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "9dgCP_vbeO_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#isolation forest for anamoly detection of reviewer follower\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n",
        "isolation_forest.fit(merged['Reviewer_Followers'].values.reshape(-1, 1))\n",
        "merged['anomaly_score_univariate_follower'] = isolation_forest.decision_function(\n",
        "    merged['Reviewer_Followers'].values.reshape(-1, 1))\n",
        "merged['outlier_univariate_follower'] = isolation_forest.predict(\n",
        "    merged['Reviewer_Followers'].values.reshape(-1, 1))\n"
      ],
      "metadata": {
        "id": "OxxDN0BseiKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chat to visualize outliers in reviwer follower column\n",
        "xx = np.linspace(merged['Reviewer_Followers'].min(), merged['Reviewer_Followers'].max(), len(merged)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Reviewer_Followers')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "y5ekWuKqeohP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To separate the symmetric distributed features and skew symmetric distributed features\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in merged.describe().columns:\n",
        "  if abs(merged[i].mean()-merged[i].median())<0.2:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "8bY5IIUXfCem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Skew Symmetric features defining upper and lower boundry\n",
        "#Outer Fence\n",
        "def outlier_treatment_skew(df,feature):\n",
        "  IQR= df[feature].quantile(0.75)- df[feature].quantile(0.25)\n",
        "  lower_bridge =df[feature].quantile(0.25)- 1.5*IQR\n",
        "  upper_bridge =df[feature].quantile(0.75)+ 1.5*IQR\n",
        "  # print(f'upper : {upper_bridge} lower : {lower_bridge}')\n",
        "  return upper_bridge,lower_bridge\n"
      ],
      "metadata": {
        "id": "XytqJHi5fRMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundary for cost in restaurant dataset\n",
        "#lower limit capping\n",
        "meta.loc[meta['Cost']<= outlier_treatment_skew(df=meta,\n",
        "  feature='Cost')[1], 'Cost']=outlier_treatment_skew(df=meta,feature='Cost')[1]\n",
        "\n",
        "#upper limit capping\n",
        "meta.loc[meta['Cost']>= outlier_treatment_skew(df=meta,\n",
        "  feature='Cost')[0], 'Cost']=outlier_treatment_skew(df=meta,feature='Cost')[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "WfRv8wtUfW5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Restricting the data to lower and upper boundary for cost in review dataset\n",
        "#lower limit capping\n",
        "review.loc[review['Reviewer_Followers']<= outlier_treatment_skew(df=review,\n",
        "  feature='Reviewer_Followers')[1], 'Reviewer_Followers']=outlier_treatment_skew(\n",
        "      df=review,feature='Reviewer_Followers')[1]\n",
        "\n",
        "#upper limit capping\n",
        "review.loc[review['Reviewer_Followers']>= outlier_treatment_skew(df=review,\n",
        "  feature='Reviewer_Followers')[0], 'Reviewer_Followers']=outlier_treatment_skew(\n",
        "      df=review,feature='Reviewer_Followers')[0]"
      ],
      "metadata": {
        "id": "a5WX2io_f3zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the columns created while outliers treatment\n",
        "merged.drop(columns =['anomaly_score_univariate_Cost','outlier_univariate_Cost',\n",
        "  'anomaly_score_univariate_follower','outlier_univariate_follower'], inplace = True)"
      ],
      "metadata": {
        "id": "ITEKLmzTgUa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Since cost and reviewer follower feature or column show positive skewed distribution and using isolation forest found they have outliers, hence using the capping technique instead of removing the outliers, capped outliers with the highest and lowest limit using IQR method."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new df with important categories\n",
        "cluster_dummy = meta[['Restaurant','Cuisines']]\n",
        "#spliting cuisines as they are separted with comma and converting into list\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].str.split(',')\n",
        "#using explode converting list to unique individual items\n",
        "cluster_dummy = cluster_dummy.explode('Cuisines')\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].apply(lambda x: x.strip())\n",
        "#using get dummies to get dummies for cuisines\n",
        "cluster_dummy = pd.get_dummies(cluster_dummy, columns=[\"Cuisines\"], prefix=[\"Cuisines\"])"
      ],
      "metadata": {
        "id": "wMkV2BJwgtnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if the values are correct\n",
        "cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].idxmax(1)[:6]\n"
      ],
      "metadata": {
        "id": "8iwObzt9g_6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing cuisines_ from columns name - for better understanding run seperatly\n",
        "\n",
        "# cluster_dummy.columns = cluster_dummy.columns.str.replace(\" \",\"\")\n",
        "cluster_dummy.columns = cluster_dummy.columns.str.replace(\"Cuisines_\",\"\")\n",
        "# cluster_dummy = cluster_dummy.groupby(cluster_dummy.columns, axis=1).sum()\n",
        "\n",
        "#grouping each restaurant as explode created unnecessary rows\n",
        "cluster_dummy = cluster_dummy.groupby(\"Restaurant\").sum().reset_index()\n"
      ],
      "metadata": {
        "id": "D9XWAINYhHfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#total cuisine count\n",
        "meta['Total_Cuisine_Count'] = meta['Cuisines'].apply(lambda x : len(x.split(',')))\n"
      ],
      "metadata": {
        "id": "QcQfNUo_hhjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding average rating - will remove 5 unrated restaurant from 105 restaurant\n",
        "avg_hotel_rating.rename(columns = {'Rating':'Average_Rating'}, inplace =True)\n",
        "meta = meta.merge(avg_hotel_rating[['Average_Rating','Restaurant']], on = 'Restaurant')\n",
        "meta.head(1)"
      ],
      "metadata": {
        "id": "bdes4SZlhxZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding cost column to the new dataset\n",
        "cluster_dummy = meta[['Restaurant','Cost','Average_Rating','Total_Cuisine_Count'\n",
        "                      ]].merge(cluster_dummy, on = 'Restaurant')"
      ],
      "metadata": {
        "id": "yT4d2oaliJlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy.shape"
      ],
      "metadata": {
        "id": "L-P3FP20iR8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating data frame for categorial encoding\n",
        "cluster_df = meta[['Restaurant','Cuisines','Cost','Average_Rating','Total_Cuisine_Count']]\n"
      ],
      "metadata": {
        "id": "PX4e55Q5ip6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new dataframe for clustering\n",
        "cluster_df = pd.concat([cluster_df,pd.DataFrame(columns=list(cuisine_dict.keys()))])\n"
      ],
      "metadata": {
        "id": "tWtsdGRaiyn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating categorial feature for cuisine\n",
        "#iterate over every row in the dataframe\n",
        "for i, row in cluster_df.iterrows():\n",
        "  # iterate over the new columns\n",
        "  for column in list(cluster_df.columns):\n",
        "      if column not in ['Restaurant','Cost','Cuisines','Average_Rating','Total_Cuisine_Count']:\n",
        "        # checking if the column is in the list of cuisines available for that row\n",
        "        if column in row['Cuisines']:\n",
        "          #assign it as 1 else 0\n",
        "          cluster_df.loc[i,column] = 1\n",
        "        else:\n",
        "          cluster_df.loc[i,column] = 0"
      ],
      "metadata": {
        "id": "3VR4dsvri5OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.head(2).T"
      ],
      "metadata": {
        "id": "rvlxkWjCjH9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I have used one hot encoding on the cuisine category and based on the cuisine if present i gave value to 1 and if absent gave value of 0. Benefit of using one hot encoding."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REVIEW DATA\n",
        "#creating new df for text processing of sentiment analysis\n",
        "sentiment_df = review[['Reviewer','Restaurant','Rating','Review']]\n",
        "#analysing two random sample\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "sWRdiR-sjzGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting index\n",
        "sentiment_df = sentiment_df.reset_index()\n",
        "sentiment_df['index'] = sentiment_df.index"
      ],
      "metadata": {
        "id": "6SA_tWcZkOzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "WgBfWBpmkYQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "# applying fuction for contracting text\n",
        "sentiment_df['Review']=sentiment_df['Review'].apply(lambda x:contractions.fix(x))\n"
      ],
      "metadata": {
        "id": "yuZs1rZGksO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'] = sentiment_df['Review'].str.lower()\n"
      ],
      "metadata": {
        "id": "cTd4HrN5k14D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "vgqYF-sqk9Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def remove_punctuation(text):\n",
        "  '''a function for removing punctuation'''\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  return text.translate(translator)"
      ],
      "metadata": {
        "id": "7BwFtP8hlJV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'] = sentiment_df['Review'].apply(remove_punctuation)\n",
        "sentiment_df.sample(5)"
      ],
      "metadata": {
        "id": "lmnwCXIaljqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Remove links\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "\n",
        "# Remove digits\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"\\d+\", \"\", x))"
      ],
      "metadata": {
        "id": "JXHeZ-X2l0IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#function to extract location of the restaurant\n",
        "def get_location(link):\n",
        "  link_elements = link.split(\"/\")\n",
        "  return link_elements[3]\n",
        "\n",
        "#create a location feature\n",
        "meta['Location'] = meta['Links'].apply(get_location)\n",
        "meta.sample(2)"
      ],
      "metadata": {
        "id": "vG3WKM-5l_z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "sw = stopwords.words('english')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove stopwords\n",
        "def delete_stopwords(text):\n",
        "  '''a function for removing the stopword'''\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "  # joining the list of words with space separator\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "LCf4CKDBv4AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'] = sentiment_df['Review'].apply(delete_stopwords)"
      ],
      "metadata": {
        "id": "q49ryjD4wIsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'] =sentiment_df['Review'].apply(lambda x: \" \".join(x.split()))\n"
      ],
      "metadata": {
        "id": "r9V2WBn_wQTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)\n"
      ],
      "metadata": {
        "id": "hIMvfB0awVqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df['Review'] = sentiment_df['Review'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "x5WmIOoNwkiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "KGsOifmJwwSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create a lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Lemmatize the 'Review' column\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(lemmatize_tokens)\n"
      ],
      "metadata": {
        "id": "gYUZrn3FxELT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "BliGSVXOxNwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I have used lemmatization because it is a more accurate way of reducing words to their base form than stemming. Lemmatization considers the context of the word and its grammatical structure to determine its base form, which can help to improve the performance of natural language processing models."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "vectorizer.fit(sentiment_df['Review'].values)\n",
        "#creating independent variable for sentiment analysis\n",
        "X_tfidf = vectorizer.transform(sentiment_df['Review'].values)"
      ],
      "metadata": {
        "id": "SwluRZVuxrMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of Words\n",
        "tokenized_text = []\n",
        "for token in sentiment_df['Review']:\n",
        "    tokenized_text.append(token)\n",
        "\n",
        "#creating token dict\n",
        "tokens_dict = gensim.corpora.Dictionary(tokenized_text)\n"
      ],
      "metadata": {
        "id": "9J9g3J49x34_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using tokens_dict.doc2bow() to generate BoW features for each tokenized course\n",
        "texts_bow = [tokens_dict.doc2bow(text) for text in tokenized_text]\n",
        "\n",
        "#creating a new text_bow dataframe based on the extracted BoW features\n",
        "tokens = []\n",
        "bow_values = []\n",
        "doc_indices = []\n",
        "doc_ids = []\n",
        "for text_idx, text_bow in enumerate(texts_bow):\n",
        "    for token_index, token_bow in text_bow:\n",
        "        token = tokens_dict.get(token_index)\n",
        "        tokens.append(token)\n",
        "        bow_values.append(token_bow)\n",
        "        doc_indices.append(text_idx)\n",
        "        doc_ids.append(sentiment_df[\"Restaurant\"][text_idx])\n",
        "\n",
        "bow_dict = {\"doc_index\": doc_indices,\n",
        "            \"doc_id\": doc_ids,\n",
        "            \"token\": tokens,\n",
        "            \"bow\": bow_values,\n",
        "            }"
      ],
      "metadata": {
        "id": "0dozldAVyF6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bows_df = pd.DataFrame(bow_dict)\n",
        "bows_df.head()\n"
      ],
      "metadata": {
        "id": "UlFvtNYHyKle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "TF-IDF (term frequency-inverse document frequency) is a technique that assigns a weight to each word in a document. It is calculated as the product of the term frequency (tf) and the inverse document frequency (idf)."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Restaurant\n",
        "meta.shape"
      ],
      "metadata": {
        "id": "EnhH922yyzQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta.columns"
      ],
      "metadata": {
        "id": "bY97zSoAzBGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta = meta.drop(columns = ['Links','Location'], axis = 1)\n"
      ],
      "metadata": {
        "id": "39DWGTQPzLNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta.shape"
      ],
      "metadata": {
        "id": "9p8RPY0HzbaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this dataset was used earlier while categorial encoding hence using it for clustering\n",
        "cluster_df.shape"
      ],
      "metadata": {
        "id": "r956ihsKzpXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df = cluster_df.drop(columns = ['Restaurant','Cuisines'], axis = 1)\n"
      ],
      "metadata": {
        "id": "J1_-ibOYzvFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.sample(1)"
      ],
      "metadata": {
        "id": "l1f-asZ_z2b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy.shape"
      ],
      "metadata": {
        "id": "GKd0I_cFz-km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#review data shape\n",
        "review.shape"
      ],
      "metadata": {
        "id": "_4GFvpDo0PMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new binary feature called sentiment based on rating which has 1 = positive and 0 = negative\n",
        "sentiment_df['Sentiment'] = sentiment_df['Rating'].apply(\n",
        "    lambda x: 1 if x >=sentiment_df['Rating'].mean() else 0)  #1 = positive # 0 = negative\n"
      ],
      "metadata": {
        "id": "_RBqzeFF0SV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)\n"
      ],
      "metadata": {
        "id": "O4GA5mkq0f4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta.columns"
      ],
      "metadata": {
        "id": "TssQSOB81RzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.columns"
      ],
      "metadata": {
        "id": "Efy56y2A1ZN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy.columns"
      ],
      "metadata": {
        "id": "TTGue61r1hvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review.columns"
      ],
      "metadata": {
        "id": "HXl_LaoK1peh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.columns\n"
      ],
      "metadata": {
        "id": "ysvr3bZA1x2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "PCA for feature selection, which will be again beneficial for dimensional reduction, therefore will do the needfull in the precedding step."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The goal of PCA is to identify the most important variables or features that capture the most variation in the data, and then to project the data onto a lower-dimensional space while preserving as much of the variance as possible."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting symmetric and skew symmetric features from the cplumns\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in cluster_df.describe().columns:\n",
        "  if abs(cluster_df[i].mean()-cluster_df[i].median())<0.1:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "bSGjHnIC2eCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using log transformation to transform Cost as using capping tends to change median and mean\n",
        "cluster_df['Cost'] = np.log1p(cluster_df['Cost'])\n",
        "cluster_dummy['Cost'] = np.log1p(cluster_dummy['Cost'])"
      ],
      "metadata": {
        "id": "u1poZh_w2q2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,col in enumerate(['Cost']) :\n",
        "    sns.distplot(cluster_df[col], color = '#055E85', fit = norm);\n",
        "    feature = cluster_df[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');  #red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median'); #cyan\n",
        "    plt.legend(bbox_to_anchor = (1.0, 1), loc = 'upper right')\n",
        "    plt.title(f'{col.title()}');\n",
        "    plt.tight_layout();"
      ],
      "metadata": {
        "id": "kYO67rK323t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy.sample(5)"
      ],
      "metadata": {
        "id": "hVeqr7RvRBqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = ['Cost','Total_Cuisine_Count','Average_Rating']\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(cluster_dummy[numerical_cols])\n",
        "scaled_df = cluster_dummy.copy()\n",
        "scaled_df[numerical_cols] = scaler.transform(cluster_dummy[numerical_cols])\n"
      ],
      "metadata": {
        "id": "neuDL8QW3mf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        " I have used standard scaler as those numerical columns where normally distributed."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#applying pca\n",
        "#setting restaurant feature as index as it still had categorial value\n",
        "scaled_df.set_index(['Restaurant'],inplace=True)\n",
        "features = scaled_df.columns\n",
        "# features = features.drop('Restaurant')\n",
        "# create an instance of PCA\n",
        "pca = PCA()\n",
        "\n",
        "# fit PCA on features\n",
        "pca.fit(scaled_df[features])\n"
      ],
      "metadata": {
        "id": "0bBRv0-z4MhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explained variance v/s no. of components\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker ='o', color = 'orange')\n",
        "plt.xlabel('number of components',size = 15, color = 'red')\n",
        "plt.ylabel('cumulative explained variance',size = 14, color = 'blue')\n",
        "plt.title('Variance v/s No. of Components',size = 20, color = 'green')\n",
        "plt.xlim([0, 8])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BEJ-g9An4c4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using n_component as 3\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "# fit PCA on features\n",
        "pca.fit(scaled_df[features])\n",
        "\n",
        "# explained variance ratio of each principal component\n",
        "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "# variance explained by three components\n",
        "print('Cumulative variance explained by 3 principal components: {:.2%}'.format(\n",
        "                                        np.sum(pca.explained_variance_ratio_)))\n",
        "\n",
        "# transform data to principal component space\n",
        "df_pca = pca.transform(scaled_df[features])"
      ],
      "metadata": {
        "id": "_JRcAmnB4zsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"original shape: \", scaled_df.shape)\n",
        "print(\"transformed shape:\", df_pca.shape)\n"
      ],
      "metadata": {
        "id": "B-45nUmD4-dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Yes, it is important to use dimensionality reduction techniques as dataset has 40 or more features. This is because, as the number of features increases, the computational cost of clustering algorithms also increases."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# for sentiment analysis using sentiment_df dataframe\n",
        "X = X_tfidf #from text vectorization\n",
        "y = sentiment_df['Sentiment']"
      ],
      "metadata": {
        "id": "iSwFbCKo5pRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.shape"
      ],
      "metadata": {
        "id": "aaoLoICX5z7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting test train\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n"
      ],
      "metadata": {
        "id": "ZhZa8Jci57eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)\n"
      ],
      "metadata": {
        "id": "V16WbYdg6BgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I have used 80:20 split which is one the most used split ratio. Since there was only 9961 data, therefore I have used more in training set."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the value count for target class\n",
        "vc = sentiment_df.Sentiment.value_counts().reset_index().rename(columns =\n",
        "            {'index':'Sentiment','Sentiment':'Count'})\n"
      ],
      "metadata": {
        "id": "AipMeSLY6wn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining majority and minority class value\n",
        "majority_class = vc.Count[0]\n",
        "minority_class = vc.Count[1]\n"
      ],
      "metadata": {
        "id": "mgpPRTXW62k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating cir value for checking class imbalance\n",
        "CIR = majority_class / minority_class\n",
        "CIR"
      ],
      "metadata": {
        "id": "6au4oAG566jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The CIR is calculated as the ratio of the number of observations in the majority class (Nm) to the number of observations in the minority class (Nm)."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "#Within Cluster Sum of Squared Errors(WCSS) for different values of k\n",
        "wcss=[]\n",
        "for i in range(1,11):\n",
        "    km=KMeans(n_clusters=i,random_state = 20)\n",
        "    km.fit(df_pca)\n",
        "    wcss.append(km.inertia_)\n",
        "\n",
        "     #elbow curve\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.plot(range(1,11),wcss, linewidth=2, color=\"red\", marker =\"o\")\n",
        "plt.xlabel(\"K Value\", size = 20, color = 'purple')\n",
        "plt.xticks(np.arange(1,11,1))\n",
        "plt.ylabel(\"WCSS\", size = 20, color = 'green')\n",
        "plt.title('Elbow Curve', size = 20, color = 'blue')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#silhouette score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "# candidates for the number of cluster\n",
        "parameters = list(range(2,10))\n",
        "#parameters\n",
        "parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
        "best_score = -1\n",
        "#visualizing Silhouette Score for individual clusters and the clusters made\n",
        "for n_clusters in parameters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # 1st subplot is the silhouette plot\n",
        "    # silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(df_pca) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(df_pca)\n",
        "\n",
        "    # silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(df_pca, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(df_pca, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(df_pca[:, 0], df_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')"
      ],
      "metadata": {
        "id": "1L_mcrVnObXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualizing the clusters and the datapoints in each clusters\n",
        "plt.figure(figsize = (10,6), dpi = 120)\n",
        "\n",
        "kmeans= KMeans(n_clusters = 5, init= 'k-means++', random_state = 42)\n",
        "kmeans.fit(df_pca)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(df_pca)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(df_pca[label == i , 0] , df_pca[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cMOzSS6DQ76M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#making df for pca\n",
        "kmeans_pca_df = pd.DataFrame(df_pca,columns=['PC1','PC2','PC3'],index=scaled_df.index)\n",
        "kmeans_pca_df[\"label\"] = label\n",
        "kmeans_pca_df.sample(2)\n"
      ],
      "metadata": {
        "id": "g-OxvNNDRZCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joining the cluster labels to names dataframe\n",
        "cluster_dummy.set_index(['Restaurant'],inplace=True)\n",
        "cluster_dummy = cluster_dummy.join(kmeans_pca_df['label'])\n",
        "cluster_dummy.sample(2)\n"
      ],
      "metadata": {
        "id": "EzrwKwSFRgzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing back cost value to original from log1p done during transformation\n",
        "cluster_dummy['Cost'] = np.expm1(cluster_dummy['Cost'])\n",
        "cluster_dummy.sample(2)\n"
      ],
      "metadata": {
        "id": "_SRtUYo9RqWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating df to store cluster data\n",
        "clustering_result = cluster_dummy.copy().reset_index()\n",
        "clustering_result = meta[['Restaurant','Cuisines']].merge(clustering_result[['Restaurant','Cost',\n",
        "                  'Average_Rating',\t'Total_Cuisine_Count','label']], on = 'Restaurant')\n",
        "clustering_result.head()"
      ],
      "metadata": {
        "id": "hh9XBIXSRxqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting content in each cluster\n",
        "cluster_count = cluster_dummy['label'].value_counts().reset_index().rename(\n",
        "    columns={'index':'label','label':'Total_Restaurant'}).sort_values(by='Total_Restaurant')\n",
        "cluster_count\n"
      ],
      "metadata": {
        "id": "TtswvKH0R9VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new df for checkign cuising in each cluster\n",
        "new_cluster_df = clustering_result.copy()\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].str.split(',')\n",
        "new_cluster_df = new_cluster_df.explode('Cuisines')\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].apply(lambda x: x.strip())\n",
        "new_cluster_df.sample(5)\n"
      ],
      "metadata": {
        "id": "lY-zafSYSFNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing cuisine list for each cluster\n",
        "for cluster in new_cluster_df['label'].unique().tolist():\n",
        "  print('Cuisine List for Cluster :', cluster,'\\n')\n",
        "  print(new_cluster_df[new_cluster_df[\"label\"]== cluster]['Cuisines'].unique(),'\\n')\n",
        "  print('='*120)"
      ],
      "metadata": {
        "id": "NW5Y21t8SNjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing module for hierarchial clustering and vizualizing dendograms\n",
        "import scipy.cluster.hierarchy as sch\n",
        "plt.figure(figsize=(12,5))\n",
        "dendrogram = sch.dendrogram(sch.linkage(df_pca, method = 'ward'),orientation='top',\n",
        "            distance_sort='descending',\n",
        "            show_leaf_counts=True)\n",
        "\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "YjdtKO_dSXvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the Silhouette score for 15 clusters\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "for n_clusters in range_n_clusters:\n",
        "    hc = AgglomerativeClustering(n_clusters = n_clusters, affinity = 'euclidean', linkage = 'ward')\n",
        "    y_hc = hc.fit_predict(df_pca)\n",
        "    score = silhouette_score(df_pca, y_hc)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n"
      ],
      "metadata": {
        "id": "i4ExAXNqSriY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agglomerative clustering\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# define the model\n",
        "model = AgglomerativeClustering(n_clusters = 5)      #n_clusters=5\n",
        "# fit model and predict clusters\n",
        "y_hc = model.fit_predict(df_pca)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(y_hc)\n",
        "# create scatter plot for samples from each cluster\n",
        "for cluster in clusters:\n",
        "\t# get row indexes for samples with this cluster\n",
        "\trow_ix = where(y_hc == cluster)\n",
        "\t# create scatter of these samples\n",
        "\tplt.scatter(df_pca[row_ix, 0], df_pca[row_ix, 1])\n",
        "# show the plot\n",
        "plt.show()\n",
        "#Evaluation\n",
        "\n",
        "#Silhouette Coefficient\n",
        "print(\"Silhouette Coefficient: %0.3f\"%silhouette_score(df_pca,y_hc, metric='euclidean'))\n",
        "\n",
        "#davies_bouldin_score of our clusters\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "davies_bouldin_score(df_pca, y_hc)\n",
        "print(\"davies_bouldin_score %0.3f\"%davies_bouldin_score(df_pca, y_hc))\n"
      ],
      "metadata": {
        "id": "Qn1Z5ulUS2IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new colummn for predicting cluster using hierarcial clsutering\n",
        "clustering_result['label_hr'] = y_hc\n"
      ],
      "metadata": {
        "id": "cVfY1F4iTLtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_result.sample(5)"
      ],
      "metadata": {
        "id": "6NtHFnjlTQaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-MEANS\n",
        "I applied K means Clustering to cluster the Restaurants based on the given features. I used both the Elbow and Silhuoette Methods to get an efficient number of K, and we discovered that n clusters = 6 was best for our model. The model was then fitted using K means, and each data point was labelled with the cluster to which it belonged using K means.labels."
      ],
      "metadata": {
        "id": "lfjPm11rTsgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AGGLOMERATIVE APPROACH\n",
        "\n",
        "I have used Hierarchial Clustering - Agglomerative Model to cluster the restaurants based on different features. This model uses a down-top approach to cluster the data. I have used Silhouette Coefficient Score and used clusters = 6 and then vizualized the clusters and the datapoints within it."
      ],
      "metadata": {
        "id": "4kI3QTSJT-5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNSUPERVISED SENTIMENTAL ANALYSIS\n",
        "\n",
        "#calculating silhouette score for n_component\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "topic_range = range(2, 11)\n",
        "silhouette_scores = []\n",
        "\n",
        "for n_components in topic_range:\n",
        "    lda = LatentDirichletAllocation(n_components=n_components)\n",
        "    lda.fit(X)\n",
        "    labels = lda.transform(X).argmax(axis=1)\n",
        "    silhouette_scores.append(silhouette_score(X, labels))\n"
      ],
      "metadata": {
        "id": "mNUmKjODUgAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#plotting silhouette score\n",
        "plt.plot(topic_range, silhouette_scores, marker ='o', color='red')\n",
        "plt.xlabel('Number of Topics', size = 15, color = 'green')\n",
        "plt.ylabel('Silhouette Score', size = 15, color = 'blue')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iBRB0TNWWFDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "lda = LatentDirichletAllocation(n_components=4)\n",
        "lda.fit(X)\n"
      ],
      "metadata": {
        "id": "UxmkQwMQWUe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis==2.1.2"
      ],
      "metadata": {
        "id": "_KQlISj-k068"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using pyldavis to visualise\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()\n"
      ],
      "metadata": {
        "id": "FGk19G8EW6rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#using pyldavis to visualise\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "n22Ao2nPqUbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "Mbs2NRCKrjNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating copy to store predicted sentiments\n",
        "review_sentiment_prediction = review[review_df.columns.to_list()].copy()\n",
        "review_sentiment_prediction.head()"
      ],
      "metadata": {
        "id": "YGeH6P0GqisF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting the sentiments and storing in a feature\n",
        "topic_results = lda.transform(X)\n",
        "review_sentiment_prediction['Prediction'] = topic_results.argmax(axis=1)\n",
        "review_sentiment_prediction.sample(5)\n"
      ],
      "metadata": {
        "id": "zhy1H4GBqmSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentiment in review_sentiment_prediction['Prediction'].unique().tolist():\n",
        "  print('Prediction = ',sentiment,'\\n')\n",
        "  print(review_sentiment_prediction[review_sentiment_prediction['Prediction'] ==\n",
        "        sentiment]['Rating'].value_counts())\n",
        "  print('='*120)\n"
      ],
      "metadata": {
        "id": "OAazmoDPr_TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining function to calculate score\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from tabulate import tabulate\n",
        "import itertools\n",
        "\n",
        "\n",
        "#calculating score\n",
        "def calculate_scores(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    # Get the confusion matrix for both train and test\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.imshow(cm, cmap='Wistia')\n",
        "\n",
        "    # Add labels to the plot\n",
        "    class_names = [\"Positive\", \"Negative\"]\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Add values inside the confusion matrix\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Add a title and x and y labels\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "\n",
        "    plt.show()\n",
        "    print(cm)\n",
        "    return roc_auc, f1, accuracy, precision, recall\n",
        "\n",
        "#printing result\n",
        "def print_table(model, X_train, y_train, X_test, y_test):\n",
        "    roc_auc, f1, accuracy, precision, recall = calculate_scores(model, X_train, y_train, X_test, y_test)\n",
        "    table = [[\"ROC AUC\", roc_auc], [\"Precision\", precision],\n",
        "             [\"Recall\", recall], [\"F1\", f1], [\"Accuracy\", accuracy]]\n",
        "    print(tabulate(table, headers=[\"Metric\", \"Score\"]))"
      ],
      "metadata": {
        "id": "ry13PgK5seOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logisctic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# create and fit the model\n",
        "clf = LogisticRegression()\n"
      ],
      "metadata": {
        "id": "ZwV8vjrGsn8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XgBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#create and fit the model\n",
        "xgb = XGBClassifier()"
      ],
      "metadata": {
        "id": "2yTIMAcmsxG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# logistic regression\n",
        "# printing result\n",
        "print_table(clf, X_train, y_train, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOOST\n",
        "# Visualizing evaluation Metric Score chart for XgBoost\n",
        "# printing result\n",
        "print_table(xgb, X_train, y_train, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "iN_uSUiBtS7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression\n",
        "# finding the best parameters for LogisticRegression by gridsearchcv\n",
        "param_dict = {'C': [0.1,1,10,100,1000],'penalty': ['l1', 'l2'],'max_iter':[1000]}\n",
        "clf_grid = GridSearchCV(clf, param_dict,n_jobs=1, cv=5, verbose = 5,scoring='recall')\n"
      ],
      "metadata": {
        "id": "1ScjA2jSt67R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing result\n",
        "print_table(clf_grid, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "yza2EXcRuCVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Xgboost regresser\n",
        "# finding the best parameters for XGBRegressor by gridsearchcv\n",
        "xgb_param={'n_estimators': [100,125,150],'max_depth': [7,10,15],'criterion': ['entropy']}\n",
        "xgb_grid=GridSearchCV(estimator=xgb,param_grid = xgb_param,cv=3,scoring='recall',verbose=5,n_jobs = 1)\n"
      ],
      "metadata": {
        "id": "iaVNJrv-vd62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# printing result for gridsearch Xgb\n",
        "print_table(xgb_grid, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "MTOkLAjwvsvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting graph\n",
        "from sklearn.metrics import roc_curve\n",
        "# finding the best parameters for all the models\n",
        "log_reg_best = clf_grid.best_estimator_\n",
        "xgbc_best = xgb_grid.best_estimator_\n",
        "\n",
        "# predicting the sentiment by all models\n",
        "y_preds_proba_lr = log_reg_best.predict_proba(X_test)[::,1]\n",
        "y_preds_proba_xgbc = xgbc_best.predict_proba(X_test)[::,1]\n",
        "\n",
        "classifiers_proba = [(log_reg_best, y_preds_proba_lr),\n",
        "                    (xgbc_best, y_preds_proba_xgbc)]\n",
        "\n",
        "# Define a result table as a DataFrame\n",
        "result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
        "\n",
        "# Train the models and record the results\n",
        "for pair in classifiers_proba:\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test,  pair[1])\n",
        "    auc = roc_auc_score(y_test, pair[1])\n",
        "\n",
        "    result_table = result_table.append({'classifiers':pair[0].__class__.__name__,\n",
        "                                        'fpr':fpr,\n",
        "                                        'tpr':tpr,\n",
        "                                        'auc':auc}, ignore_index=True)\n",
        "\n",
        "# Set name of the classifiers as index labels\n",
        "result_table.set_index('classifiers', inplace=True)\n",
        "\n",
        "# ploting the roc auc curve for all models\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "\n",
        "for i in result_table.index:\n",
        "    plt.plot(result_table.loc[i]['fpr'],\n",
        "             result_table.loc[i]['tpr'],\n",
        "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
        "\n",
        "plt.plot([0,1], [0,1],'r--')\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "\n",
        "plt.title('ROC AUC Curve', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size':13}, loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "122w-a-kzh28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "XG Boost classifier\n",
        "\n",
        "after evaluation\n",
        "\n",
        "ROC AUC -0.818059\n",
        "Precision -0.848111\n",
        "Recall -0.894309\n",
        "F1 -0.870598\n",
        "Accuracy -0.835926"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The XgBoost Classifier can be considered as an efficient model for the business, especially when it achieves high scores in all of these evaluation metrics, which would indicate that it can accurately predict outcomes, identify all positive instances, and correctly classify instances as positive or negative."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Content-Based Filtering\n",
        "\n",
        "#creating variable that contain restaurant cuisine details\n",
        "restaurant_df = cluster_dummy.copy()\n",
        "restaurant_df = restaurant_df.reset_index()\n",
        "restaurant_df = restaurant_df.drop(columns = ['Cost',\t'Average_Rating',\t'Total_Cuisine_Count','label'], axis =1)\n",
        "restaurant_df.head(2)"
      ],
      "metadata": {
        "id": "dcwbSL8JgBUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#restaurant matrix\n",
        "rest_genre = restaurant_df.loc[:, restaurant_df.columns != 'Restaurant']\n",
        "rest_matrix = rest_genre.values\n",
        "rest_matrix"
      ],
      "metadata": {
        "id": "sedHV4-MgYnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating user or reviewer profile\n",
        "user_df = sentiment_df[['Reviewer',\t'Restaurant',\t'Rating']].copy()\n",
        "user_df.head()"
      ],
      "metadata": {
        "id": "OC3rSQihgf8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grouping the data by the 'user' column\n",
        "grouped_data = user_df.groupby('Reviewer')\n",
        "\n",
        "# defining a function to create the new dataframe\n",
        "def create_new_column(data):\n",
        "    return [{'Restaurant': row['Restaurant'], 'Rating': row['Rating']} for _, row in data.iterrows()]\n",
        "    #variable _ is used to store the index value, which is not used in the loop\n",
        "\n",
        "# applying the function to the grouped data and creating a new dataframe\n",
        "user_rating = grouped_data.apply(create_new_column)\n",
        "user_rating = user_rating.reset_index().rename(columns ={0:'Rated_Restaurant'})\n",
        "user_rating.head()\n"
      ],
      "metadata": {
        "id": "Xrna8gEmgplD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iterating over user rating df such that it end up making an array which had same shape as restaurant df\n",
        "user_rated_restaurant = {}\n",
        "for index, row in user_rating.iterrows():\n",
        "    user_rated_restaurant[row['Reviewer']] = {}\n",
        "    for i in range(len(row['Rated_Restaurant'])):\n",
        "        user_rated_restaurant[row['Reviewer']][row['Rated_Restaurant'][i][\n",
        "            'Restaurant']] = row['Rated_Restaurant'][i]['Rating']\n",
        "\n",
        "# creating an empty user preference vector for each user\n",
        "user_preference_vector = pd.DataFrame(np.zeros((len(user_rating), len(restaurant_df))),\n",
        "                      columns=restaurant_df.Restaurant, index=user_rating['Reviewer'])\n",
        "\n",
        "# Iterate through the user rating dataframe\n",
        "for index, row in user_rating.iterrows():\n",
        "    for i in range(len(row['Rated_Restaurant'])):\n",
        "        restaurant = row['Rated_Restaurant'][i]['Restaurant']\n",
        "        rating = row['Rated_Restaurant'][i]['Rating']\n",
        "        user_preference_vector.loc[row['Reviewer'], restaurant] = rating\n",
        "\n",
        "#reset index\n",
        "user_preference_vector = user_preference_vector.reset_index()\n"
      ],
      "metadata": {
        "id": "Dz7xatUkgzrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting output\n",
        "user_preference_vector.sample(5)\n"
      ],
      "metadata": {
        "id": "64zjn7zEhAg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using dot multiplication to find score or weight for each reviewer\n",
        "result_df = pd.DataFrame(columns = rest_genre.columns)\n",
        "for index, row in user_preference_vector.iterrows():\n",
        "    user_preference_vector_array = row[1:].values.reshape(1,-1)\n",
        "    dot_product = np.dot(user_preference_vector_array, rest_matrix)\n",
        "    result_df = result_df.append(pd.DataFrame(dot_product, columns = rest_genre.columns, index = [row['Reviewer']]))\n",
        "\n",
        "result_df = result_df.reset_index().rename(columns ={'index':'Reviewer'})"
      ],
      "metadata": {
        "id": "G9eBc2q7hI7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting output\n",
        "result_df[:5]"
      ],
      "metadata": {
        "id": "nJ-p5nxciZ5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating test user\n",
        "test_user_ids = user_rating.copy()\n",
        "test_user_ids['Rated_Restaurant_Count'] = test_user_ids['Rated_Restaurant'].apply(lambda x: len(x))\n",
        "\n",
        "#taking 1000 user who atleast rating 2 restaurant as they show repeatition\n",
        "test_user_ids = test_user_ids.sort_values('Rated_Restaurant_Count', ascending = False)[:1000]\n",
        "test_user_ids.head()"
      ],
      "metadata": {
        "id": "EY-NsTxriiee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating list for all reviewer in test ids\n",
        "test_user_ids = test_user_ids['Reviewer'].to_list()\n",
        "print(f\"Total numbers of test users {len(test_user_ids)}\")"
      ],
      "metadata": {
        "id": "1Ufe80CxipqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test user profile\n",
        "test_user_profile = result_df[result_df['Reviewer']=='Ankita']\n",
        "test_user_profile\n",
        "\n"
      ],
      "metadata": {
        "id": "TshM5_ymi0RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's get the test user vector by excluding the `user` column\n",
        "test_user_vector = test_user_profile.iloc[0, 1:].values\n",
        "test_user_vector"
      ],
      "metadata": {
        "id": "qs6Bv_G9jYyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let test reviewer or user be 'Ankita'\n",
        "liked_restaurant = user_df[user_df['Reviewer'] == 'Ankita']['Restaurant'].to_list()\n",
        "liked_restaurant = set(liked_restaurant)\n",
        "liked_restaurant"
      ],
      "metadata": {
        "id": "SuQR2gBYjgA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting values for all restaurant\n",
        "all_restaurant = set(restaurant_df['Restaurant'].values)\n"
      ],
      "metadata": {
        "id": "tTkKAhsyjnjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting unknown restaurants\n",
        "unknown_restaurant = all_restaurant.difference(liked_restaurant)\n"
      ],
      "metadata": {
        "id": "UvKn6S2Ajs79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting unknown restaurant genre\n",
        "unknown_restaurant_genres = restaurant_df[restaurant_df['Restaurant'].isin(unknown_restaurant)]\n",
        "#getting the restaurant matrix by excluding `Restaurant' columns:\n",
        "restaurant_matrix = unknown_restaurant_genres.iloc[:, 1:].values\n",
        "restaurant_matrix"
      ],
      "metadata": {
        "id": "5goTawAvjzVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#recommendation score\n",
        "score = np.dot(restaurant_matrix[1], test_user_vector)\n",
        "score"
      ],
      "metadata": {
        "id": "Tv8FrV8Fj5tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The threshold can be fine-tuned to adjust the size of generated recommendations\n",
        "score_threshold = 10.0\n",
        "# score_threshold = 20.0\n",
        "res_dict = {}"
      ],
      "metadata": {
        "id": "EsP5rGQBkECF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendation_scores():\n",
        "    users = []\n",
        "    restaurant = []\n",
        "    scores = []\n",
        "    for user_id in test_user_ids:\n",
        "        test_user_profile = result_df[result_df['Reviewer'] == user_id]\n",
        "        # get user vector for the current user id\n",
        "        test_user_vector = test_user_profile.iloc[0, 1:].values\n",
        "         # get the unknown restaurant ids for the current user id\n",
        "        liked_restaurant = user_df[user_df['Reviewer'] == user_id]['Restaurant'].to_list()\n",
        "        all_restaurant = set(restaurant_df['Restaurant'].values)\n",
        "        unknown_restautant = all_restaurant.difference(liked_restaurant)\n",
        "        unknown_restaurant_genres = restaurant_df[restaurant_df['Restaurant'].isin(unknown_restaurant)]\n",
        "        unknown_restaurant_ids = unknown_restaurant_genres.iloc[:, :1].values\n",
        "\n",
        "        # user np.dot() to get the recommendation scores for each restaurant\n",
        "        recommendation_scores = np.dot(unknown_restaurant_genres.iloc[:, 1:].values, test_user_vector)\n",
        "\n",
        "        # Append the results into the users, restaurant, and scores list\n",
        "        for i in range(0, len(unknown_restaurant_ids)):\n",
        "            score = recommendation_scores[i]\n",
        "            # Only keep the restaurant with high recommendation score\n",
        "            if score >= score_threshold:\n",
        "              users.append(user_id)\n",
        "              restaurant.append(unknown_restaurant_ids[i])\n",
        "              scores.append(recommendation_scores[i])\n",
        "\n",
        "    return users, restaurant, scores\n"
      ],
      "metadata": {
        "id": "sMYHtWp7kKo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return users, courses, and scores lists for the dataframe\n",
        "users, restaurant, scores = generate_recommendation_scores()\n",
        "res_dict['User'] = users\n",
        "res_dict['Restaurant'] = restaurant\n",
        "res_dict['Score'] = scores\n",
        "res_df = pd.DataFrame(res_dict, columns=['User', 'Restaurant', 'Score'])\n",
        "res_df['Restaurant'] = res_df['Restaurant'].apply(lambda x: str(x[0]))\n",
        "res_df"
      ],
      "metadata": {
        "id": "kCfxQ9pykpDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#most recommended restaurant\n",
        "recom_rest = res_df.groupby('Restaurant')['User'].count().reset_index().sort_values(\n",
        "                            'User', ascending = False)\n",
        "recom_rest[:5]\n"
      ],
      "metadata": {
        "id": "cA7fXUI9kz6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#least recommended restaurant\n",
        "recom_rest[-5:]\n"
      ],
      "metadata": {
        "id": "EIC3D6IQkT-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grouping the data by the 'user' column\n",
        "grouped_data = res_df.groupby('User')\n",
        "\n",
        "# defining a function to create the new dataframe\n",
        "def create_new_column(data):\n",
        "    return [{'Restaurant': row['Restaurant'], 'Score': row['Score']} for _, row in data.iterrows()]\n",
        "    #variable _ is used to store the index value, which is not used in the loop\n",
        "\n",
        "# applying the function to the grouped data and creating a new dataframe\n",
        "recommendation = grouped_data.apply(create_new_column)\n",
        "recommendation = recommendation.reset_index().rename(columns ={0:'Recommended_Restaurant'})\n",
        "recommendation.head()"
      ],
      "metadata": {
        "id": "VQNh4h1SlCvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating column for total recommendation count for each user\n",
        "recommendation['Total_Recommendation'] = recommendation['Recommended_Restaurant'].apply(\n",
        "    lambda x: len(x))\n",
        "\n",
        "#top 10 user who get most recommendation\n",
        "recommendation.sort_values('Total_Recommendation', ascending= False)[:10]\n"
      ],
      "metadata": {
        "id": "2kaTG1eBlLIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating new dataframe for recommendation for test user\n",
        "for i in recommendation[recommendation['User']=='Ankita']['Recommended_Restaurant']:\n",
        "    # creating the dataframe\n",
        "    vis = pd.DataFrame(i, columns = ['Restaurant', 'Score'])\n",
        "vis.sort_values('Score', ascending = False)"
      ],
      "metadata": {
        "id": "E5N94o6elSkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bag of word with doc index as these index will be used for finding similarity later\n",
        "bows_df.sample(5)"
      ],
      "metadata": {
        "id": "2LUb8SQSlb4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using extracted bag of words\n",
        "bow_df = bows_df.drop(columns = ['doc_index'], axis =1)\n",
        "bow_df.head()\n"
      ],
      "metadata": {
        "id": "GtPfwDypliVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Restaurant and review\n",
        "rest_review = sentiment_df[['Restaurant','Review']].copy()\n",
        "rest_review.sample(5)\n",
        "\n"
      ],
      "metadata": {
        "id": "hV-_qM9Hlo1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bag of words for restaurant 'Asian Meal Box'\n",
        "rest_bow = bow_df[bow_df['doc_id'] == 'Asian Meal Box']\n",
        "rest_bow[:10]\n"
      ],
      "metadata": {
        "id": "QCwcictBlvDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting bow to horizontal format using pivot\n",
        "rest_bowT = rest_bow.pivot_table(index=['doc_id'], columns=['token'],\n",
        "                                  aggfunc='sum').reset_index(level=[0])\n",
        "rest_bowT"
      ],
      "metadata": {
        "id": "Ykz9Eo-Ll4cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using union set to compare two restaurant set of tokens\n",
        "def pivot_two_bows(basedoc, comparedoc):\n",
        "    base = basedoc.copy()\n",
        "    base['type'] = 'base'\n",
        "    compare = comparedoc.copy()\n",
        "    compare['type'] = 'compare'\n",
        "    # append the two token sets vertically\n",
        "    join = base.append(compare)\n",
        "    # pivot the two joined courses\n",
        "    joinT = join.pivot_table(index=['doc_id', 'type'], columns='token',\n",
        "              aggfunc='sum').fillna(0).reset_index(level=[0, 1])\n",
        "    # assign columns\n",
        "    joinT.columns = ['doc_id', 'type'] + [t[1] for t in joinT.columns][2:]\n",
        "    return joinT"
      ],
      "metadata": {
        "id": "dGEHvs9Yl_ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating two test restaurant\n",
        "rest1 = bow_df[bow_df['doc_id'] == 'Asian Meal Box']\n",
        "rest2 = bow_df[bow_df['doc_id'] == 'Biryanis And More']\n"
      ],
      "metadata": {
        "id": "GRyX5nMsmGBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vectors = pivot_two_bows(rest1, rest2)\n",
        "bow_vectors"
      ],
      "metadata": {
        "id": "Cyqs9DT9mMMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "#calculating similarity between two restaurant\n",
        "similarity = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])\n",
        "\n",
        "similarity"
      ],
      "metadata": {
        "id": "_MhY7LMRmXft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#creating function to calculate cosine similarity such that matrix can be made for each restaurant similarity\n",
        "\n",
        "# Get the list of all restaurant\n",
        "all_restaurant = rest_review['Restaurant'].unique()\n",
        "\n",
        "# Initialize the dataframe to store the similarities\n",
        "df_similarities = pd.DataFrame(columns = all_restaurant, index = all_restaurant)\n",
        "\n",
        "# Iterate over the rows and columns of the dataframe\n",
        "for i in all_restaurant:\n",
        "    for j in all_restaurant:\n",
        "        # Get the BoW representation of the current row and column restaurant\n",
        "        #creating two test restaurant\n",
        "        rest1 = bow_df[bow_df['doc_id'] == i]\n",
        "        rest2 = bow_df[bow_df['doc_id'] == j]\n",
        "        bow_vectors = pivot_two_bows(rest1, rest2)\n",
        "        # Calculate the cosine similarity between the two restaurant' BoW representations\n",
        "        sim = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])\n",
        "        # Assign the similarity score to the corresponding cell of the dataframe\n",
        "        df_similarities.at[i, j] = sim"
      ],
      "metadata": {
        "id": "MIqFQFQLmfVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_similarities"
      ],
      "metadata": {
        "id": "Q8N1v88dpOGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create restaurant id to index and index to id mappings\n",
        "def get_doc_dicts(bow_df):\n",
        "    grouped_df = bow_df.groupby(['doc_id']).max().reset_index(drop=False)\n",
        "    idx_id_dict = grouped_df[['doc_id']].to_dict()['doc_id']\n",
        "    id_idx_dict = {v: k for k, v in idx_id_dict.items()}\n",
        "    del grouped_df\n",
        "    return idx_id_dict, id_idx_dict"
      ],
      "metadata": {
        "id": "cNw69lRkpWGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#two test subject\n",
        "rest1 = rest_review[rest_review['Restaurant'] == \"Beyond Flavours\"]\n",
        "rest2 = rest_review[rest_review['Restaurant'] == \"Paradise\"]"
      ],
      "metadata": {
        "id": "jVCCPWj4pdh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with restaurant name finding index for similarity\n",
        "idx_id_dict, id_idx_dict = get_doc_dicts(bows_df)\n",
        "idx1 = id_idx_dict[\"Beyond Flavours\"]\n",
        "idx2 = id_idx_dict[\"Paradise\"]\n",
        "print(f\"Restaurant 1's index is {idx1} and Restaurant 2's index is {idx2}\")\n"
      ],
      "metadata": {
        "id": "NifDH_gzpjvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#locating in the similarti df\n",
        "sim_matrix = df_similarities.to_numpy()\n",
        "\n",
        "#similarity between the two restaurant\n",
        "sim = sim_matrix[idx1][idx2]\n",
        "sim\n"
      ],
      "metadata": {
        "id": "LmosLVOLpu0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to recommend restaurant based on similarity\n",
        "def generate_recommendations_for_one_user(liked_restaurant, unknown_restaurant, id_idx_dict, sim_matrix):\n",
        "    # Create a dictionary to store your recommendation results\n",
        "    res = {}\n",
        "    threshold = 0.6\n",
        "    for liked_rest in liked_restaurant:\n",
        "        for unselect_rest in unknown_restaurant:\n",
        "            if liked_rest in id_idx_dict and unselect_rest in id_idx_dict:\n",
        "                sim = 0\n",
        "                idx1 = id_idx_dict[liked_rest]\n",
        "                idx2 = id_idx_dict[unselect_rest]\n",
        "\n",
        "                # Find the similarity value from the sim_matrix\n",
        "                sim = sim_matrix[idx1][idx2]\n",
        "                if sim > threshold:\n",
        "                    if unselect_rest not in res:\n",
        "                        res[unselect_rest] = sim\n",
        "                    else:\n",
        "                        if sim >= res[unselect_rest]:\n",
        "                            res[unselect_rest] = sim\n",
        "\n",
        "    # Sort the results by similarity\n",
        "    res = {k: v for k, v in sorted(res.items(), key=lambda item: item[1], reverse=True)}\n",
        "    return res"
      ],
      "metadata": {
        "id": "s2a7xdGwp54e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to calculate recommendation for all Reviewer\n",
        "def generate_recommendations_for_all():\n",
        "    users = []\n",
        "    restaurant = []\n",
        "    sim_scores = []\n",
        "    idx_id_dict, id_idx_dict = get_doc_dicts(bows_df)\n",
        "    sim_matrix = df_similarities.to_numpy()\n",
        "    all_restaurant = set(restaurant_df['Restaurant'])\n",
        "    for user_id in test_user_ids:\n",
        "        liked_restaurant = user_df[user_df['Reviewer'] == user_id]['Restaurant'].to_list()\n",
        "        unknown_restaurant = all_restaurant.difference(liked_restaurant)\n",
        "        rec = generate_recommendations_for_one_user(liked_restaurant, unknown_restaurant, id_idx_dict, sim_matrix)\n",
        "        for k, v in rec.items():\n",
        "            users.append(user_id)\n",
        "            restaurant.append(k)\n",
        "            sim_scores.append(v)\n",
        "\n",
        "    return users, restaurant, sim_scores"
      ],
      "metadata": {
        "id": "EXSGQvDKqCuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing recommendation for each user in dataframe\n",
        "res_sim_dict = {}\n",
        "users, restaurant, sim_scores = generate_recommendations_for_all()\n",
        "res_sim_dict['USER'] = users\n",
        "res_sim_dict['RESTAURANT'] = restaurant\n",
        "res_sim_dict['SCORE'] = sim_scores\n",
        "res_sim_df = pd.DataFrame(res_sim_dict, columns=['USER', 'RESTAURANT', 'SCORE'])"
      ],
      "metadata": {
        "id": "4UCra9SfqJvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the output\n",
        "res_sim_df.sample(10)"
      ],
      "metadata": {
        "id": "Q499RE1uqPld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content-based filtering is a technique used in recommendation systems to recommend items to users based on their past preferences or interactions with items. It works by analyzing the attributes of the items and user preferences, and recommending items that have similar attributes."
      ],
      "metadata": {
        "id": "evmoN8csqZOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The specific evaluation metric to use will depend on the specific use case and the relative costs of false positives and false negatives. For a positive business impact, F1-score can be considered as it balances the precision and recall to give an overall performance measure."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "XGBoost's gradient-boosted decision trees algorithm can be highly effective in sentiment analysis as it can learn complex relationships between features and sentiment labels.\n",
        "\n",
        "XGBoost's ensemble nature, regularization, speed, feature selection, and gradient-boosted decision tree algorithm make it a powerful tool for sentiment analysis."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get shap values\n",
        "explainer = shap.Explainer(xgb)\n",
        "shap_values = explainer(X_test)"
      ],
      "metadata": {
        "id": "SGZtIvD0rOBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Waterfall plot for first observation\n",
        "shap.plots.waterfall(shap_values[0])\n"
      ],
      "metadata": {
        "id": "PLt2aZ8Trc5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oqwHGwgUreHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "There will be a unique waterfall plot for every observation/abalone in our dataset. They can all be interpreted in the same way as above. In each case, the SHAP values tell us how much each factor contributed to the models prediction when compared to the mean prediction. Large positive/negative SHAP values indicate that the feature had a significant impact on the models prediction."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "\n",
        "import pickle\n",
        "import os"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('model_pickle','wb') as f:\n",
        "  pickle.dump(xgb,f)"
      ],
      "metadata": {
        "id": "8PAgyJ6qtCtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "with open ('model_pickle','rb') as f:\n",
        "  xgb_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here.\n",
        "\n",
        "Overall, this analysis provided valuable insights into the customer's experience with Zomato, and it could be used to guide future business decisions and improve the service. Additionally, by combining clustering and sentiment analysis techniques, a more comprehensive understanding of customer feedback was achieved.\n",
        "\n",
        "restaurant Collage - Hyatt Hyderabad Gachibowli is most expensive restaurant in the locality which has a price of 2800 for order and has 3.5 average rating. Hotels like Amul and Mohammedia Shawarma are least expensive with price of 150 and has 3.9 average rating.\n",
        "\n",
        "Price point for high rated hotel AB's= Absolute Barbecues is 1500 and price point for low rated restaurant Hotel Zara Hi-Fi is 400.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}